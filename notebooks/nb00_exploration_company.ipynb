{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Silence pdfminer and pdfplumber logs\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pdfplumber\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slugs que siempre queremos ignorar\n",
    "EXCLUDE_SLUGS = [\n",
    "    'informacion_personal',\n",
    "    'documentacion_personal',\n",
    "    'situacion_personal_actual',\n",
    "    'composicion_familiar',\n",
    "    'datos_de_contacto',\n",
    "    'lugar_de_nacimiento',\n",
    "    'conceptorespuesta',\n",
    "    'documento_nacional_de_identidad_cedula_de_identidad',\n",
    "    'fondos_economicos',\n",
    "    'educacion',\n",
    "    'datos_del_empleador',\n",
    "    'trabajos_empleos',\n",
    "    'historial_migratorio',\n",
    "    'declaraciones_medicas',\n",
    "    'antecedentes_penales',\n",
    "    'documentos_solicitados'\n",
    "]\n",
    "\n",
    "\n",
    "# Añadimos 'ejemplo' y líneas que acaben en ':' a las instrucciones a ignorar\n",
    "INSTRUCTION_PATTERNS = [\n",
    "    r'^indicar',         # empieza con 'Indica'\n",
    "    r'^revisa',         # empieza con 'Revisa'\n",
    "    r'^este documento', # empieza con 'Este documento'\n",
    "    r'^por favor',      # empieza con 'Por favor'\n",
    "    r'^ejemplo',        # empieza con 'EJEMPLO'\n",
    "    r'^ej',             # empieza con 'Ej:'\n",
    "    r'^ten',            # empieza con 'Ten'\n",
    "    r'^\\*',             # empieza con '*',\n",
    "    r'emitió',          # contiene 'emitió' ,\n",
    "    r'^solamente',      # empieza con 'Solamente' ,\n",
    "    r'^puedes',\t        # empieza con 'Puedes' ,\n",
    "    r'^incluye'\t        # empieza con 'Incluye' ,\n",
    "    r'^experiencia',\t# empieza con 'Experiencia',\n",
    "    r'^importante'\t    # empieza con 'Importante' ,\n",
    "]\n",
    "\n",
    "ATTACHMENT_EXTENSIONS = re.compile(r'\\.(pdf|jpe?g|rtf|docx)\\b', re.IGNORECASE)\n",
    "\n",
    "def slugify(text: str) -> str:\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    text = text.lower()\n",
    "    return re.sub(r'[^a-z0-9]+', '_', text).strip('_')\n",
    "\n",
    "\n",
    "def is_instruction(text: str) -> bool:\n",
    "    low = text.lower().strip()\n",
    "    if low.endswith(':'):\n",
    "        return True\n",
    "    return any(re.search(pat, low) for pat in INSTRUCTION_PATTERNS)\n",
    "\n",
    "def sufijos_minuscula(texto):\n",
    "    # Separa en \"palabras\" (sean letras mayúsculas) ignorando espacios y caracteres como \"/\"\n",
    "    palabras = re.findall(r'[A-ZÑÁÉÍÓÚ]+', texto)\n",
    "    # Toma la primera letra de cada palabra, pásala a minúscula y únelas\n",
    "    return ''.join(p[0].lower() for p in palabras)\n",
    "\n",
    "def clean_question_for_slug(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    Si detecta una pregunta larga con '¿...?' extrae sólo lo que hay entre la última\n",
    "    abertura '¿' y el signo '?', o bien la última oración que termine en '?'\n",
    "    \"\"\"\n",
    "    q = txt.strip()\n",
    "    if '¿' in q and '?' in q:\n",
    "        start = q.rfind('¿')\n",
    "        end = q.rfind('?')\n",
    "        q = q[start+1:end]  # quita '¿' y '?'\n",
    "    elif '?' in q:\n",
    "        # toma hasta el signo '?'\n",
    "        part = q[:q.rfind('?')+1]\n",
    "        # si hay varios puntos, coge lo que viene después del último punto\n",
    "        if '.' in part:\n",
    "            q = part.split('.')[-1]\n",
    "        else:\n",
    "            q = part\n",
    "        q = q.strip(' ?')\n",
    "    # devolver sin signos innecesarios\n",
    "    return q\n",
    "\n",
    "def extract_bold_fields_with_labels(pdf_path: str):\n",
    "    data, labels = {}, {}\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages[1:]:\n",
    "            # Agrupar caracteres por línea\n",
    "            lines = {}\n",
    "            for ch in page.chars:\n",
    "                y = round(ch['top'], 1)\n",
    "                lines.setdefault(y, []).append(ch)\n",
    "            parsed = []\n",
    "            for y in sorted(lines):\n",
    "                chars = lines[y]\n",
    "                txt = ''.join(c['text'] for c in sorted(chars, key=lambda c: c['x0'])).strip()\n",
    "                fonts = [c['fontname'] for c in chars]\n",
    "                if any('Bold' in f for f in fonts):\n",
    "                    style = 'bold'\n",
    "                elif any('Light' in f for f in fonts):\n",
    "                    style = 'light'\n",
    "                else:\n",
    "                    style = 'normal'\n",
    "                parsed.append((txt, style))\n",
    "\n",
    "            suffix = None\n",
    "            for i, (txt, style) in enumerate(parsed):\n",
    "                # Detectar encabezado de sección\n",
    "                if txt.isupper() and style == 'bold':\n",
    "                    suffix = sufijos_minuscula(txt)\n",
    "\n",
    "                # Extraer campo en negrita válido\n",
    "                if style == 'bold' and not is_instruction(txt):\n",
    "                    # limpiamos la pregunta para slug\n",
    "                    clean_q = clean_question_for_slug(txt)\n",
    "                    base = slugify(clean_q)\n",
    "                    if base in EXCLUDE_SLUGS:\n",
    "                        continue\n",
    "\n",
    "                    # Construir slug con sufijo de sección si existe\n",
    "                    slug = f\"{base}_{suffix}\" if suffix else base\n",
    "                    labels[slug] = txt\n",
    "\n",
    "                    # Acumular todas las líneas hasta el siguiente bold\n",
    "                    values = []\n",
    "                    for j in range(i+1, len(parsed)):\n",
    "                        vtxt, vst = parsed[j]\n",
    "                        # si encontramos otro bold, interrumpimos\n",
    "                        if vst == 'bold':\n",
    "                            break\n",
    "                        # ignorar light e instrucciones\n",
    "                        if vst == 'light' or is_instruction(vtxt):\n",
    "                            continue\n",
    "                        values.append(vtxt)\n",
    "                    # unir con '; '\n",
    "                    data[slug] = '; '.join(values).strip()\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "def map_attachment_cell(v):\n",
    "    lv = str(v).lower().strip()\n",
    "    if re.search(ATTACHMENT_EXTENSIONS, lv):\n",
    "        return 'yes'\n",
    "    if 'no se ha subido ningún archivo' in lv:\n",
    "        return 'no'\n",
    "    return lv or pd.NA\n",
    "\n",
    "def extract_text_lines(pdf_path: str):\n",
    "    \"\"\"Read all text lines from PDF once.\"\"\"\n",
    "    lines = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages[1:]:\n",
    "            text = page.extract_text() or ''\n",
    "            lines.extend([l.strip() for l in text.split('\\n') if l.strip()])\n",
    "    return lines\n",
    "\n",
    "def process_parallel_bold(root_folder: str, field_order=None, field_labels=None):\n",
    "    \"\"\"\n",
    "    Process 'parallel' PDFs in root_folder, returning:\n",
    "      - DataFrame with one row per PDF\n",
    "      - Updated field_labels dict mapping slug -> original question text\n",
    "    \"\"\"\n",
    "    if field_order is None:\n",
    "        field_order = []\n",
    "    if field_labels is None:\n",
    "        field_labels = {}\n",
    "\n",
    "    records = []\n",
    "    entries = os.listdir(root_folder)\n",
    "    sample_dirs = [d for d in entries if os.path.isdir(os.path.join(root_folder, d))]\n",
    "    items = sorted(sample_dirs, key=lambda x: int(x) if x.isdigit() else x) if sample_dirs else sorted(entries)\n",
    "\n",
    "    for item in tqdm(items, desc=\"Processing\"):\n",
    "        # Locate PDF\n",
    "        if os.path.isdir(os.path.join(root_folder, item)):\n",
    "            folder = os.path.join(root_folder, item)\n",
    "            pdfs = [f for f in os.listdir(folder) if 'parallel' in f.lower() and f.endswith('.pdf')]\n",
    "            if not pdfs:\n",
    "                continue\n",
    "            pdf_path = os.path.join(folder, pdfs[0])\n",
    "            sample_id = item\n",
    "        else:\n",
    "            if not (item.lower().endswith('.pdf') and 'parallel' in item.lower()):\n",
    "                continue\n",
    "            pdf_path = os.path.join(root_folder, item)\n",
    "            sample_id = os.path.splitext(item)[0]\n",
    "\n",
    "        # Extract fields and labels\n",
    "        data, labels = extract_bold_fields_with_labels(pdf_path)\n",
    "\n",
    "        # Read all lines once for fallback and bullet detection\n",
    "        text_lines = extract_text_lines(pdf_path)\n",
    "\n",
    "        # Extraer MONTO:\n",
    "        monto_val = None\n",
    "        for line in text_lines:\n",
    "            m = re.search(r'\\bMONTO:\\s*(.+)', line, re.IGNORECASE)\n",
    "            if m:\n",
    "                monto_val = m.group(1).strip()\n",
    "                break\n",
    "        if monto_val is not None:\n",
    "            data['monto'] = monto_val\n",
    "            # Asegurarnos de incluirlo en el orden/etiquetas\n",
    "            if 'monto' not in field_order:\n",
    "                field_order.append('monto')\n",
    "                field_labels['monto'] = 'MONTO:'\n",
    "\n",
    "        # Update field_order and labels with new fields\n",
    "        for slug, original in labels.items():\n",
    "            if slug not in field_order:\n",
    "                field_order.append(slug)\n",
    "                field_labels[slug] = original\n",
    "\n",
    "        # Fallback: search only missing slugs in text_lines\n",
    "        for slug in field_order:\n",
    "            if slug not in data and slug in field_labels:\n",
    "                label_text = field_labels[slug].lower()\n",
    "                for idx, line in enumerate(text_lines):\n",
    "                    if label_text in line.lower():\n",
    "                        # Take next non-instruction line as value\n",
    "                        if idx + 1 < len(text_lines) and not is_instruction(text_lines[idx+1]):\n",
    "                            data[slug] = text_lines[idx+1]\n",
    "                        break\n",
    "\n",
    "        # Exclude unwanted slugs\n",
    "        for slug in EXCLUDE_SLUGS:\n",
    "            if slug in field_order:\n",
    "                field_order.remove(slug)\n",
    "            field_labels.pop(slug, None)\n",
    "\n",
    "        # Build row in fixed column order\n",
    "        row = {slug: data.get(slug) for slug in field_order}\n",
    "        row['sample_id'] = sample_id\n",
    "        records.append(row)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=field_order + ['sample_id'])\n",
    "\n",
    "    # Poner yes or no en ficheros adjuntos\n",
    "    attachment_cols = [\n",
    "        col for col, lbl in field_labels.items()\n",
    "        if re.search(r'adjuntar|copia|adjunto', lbl, re.IGNORECASE)]\n",
    "    for col in attachment_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(map_attachment_cell)\n",
    "        \n",
    "\n",
    "    # NaN en lugar de cadena vacía:\n",
    "    df.replace({'': np.nan}, inplace=True)\n",
    "    df.replace({r'^(?i:none)$': np.nan}, inplace=True)\n",
    "\n",
    "    # drops any column where *all* values are NaN\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "\n",
    "    return df, field_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_all_granted, dic_questions = process_parallel_bold('data/granted/')\n",
    "df_all_granted['final_visa_status'] = 'granted'\n",
    "\n",
    "df_all_refused, dic_questions = process_parallel_bold('data/refused/',\n",
    "                                field_order=list(dic_questions.keys()),\n",
    "                                field_labels=dic_questions)\n",
    "df_all_refused['final_visa_status'] = 'refused'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_granted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_refused.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_by_field = [i for i in df_all_refused.columns.unique() if '_ip' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all_refused[search_by_field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[i for i in df_all_refused.columns.unique() if 'ties' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all_refused[['family_ties_ds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all_refused['family_ties_ds'].loc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_all_granted,df_all_refused]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['podras_encontrar_adjunto_en_este_campo_tu_coe_certificate_of_enrolment_dp'].loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[['podras_encontrar_adjunto_en_este_campo_tu_coe_certificate_of_enrolment_dp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
