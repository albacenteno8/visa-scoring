{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from paddleocr import PaddleOCR\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Paddle\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en',show_log=False)\n",
    "\n",
    "def extract_banners_from_pdf(pdf_path):\n",
    "    super_headers = []\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        # Render page -> image\n",
    "        pix = page.get_pixmap(dpi=200)\n",
    "        img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.h, pix.w, pix.n)\n",
    "        if pix.n == 4:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY_INV)\n",
    "        cnts, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        for cnt in cnts:\n",
    "            x,y,w,h = cv2.boundingRect(cnt)\n",
    "            if w < 200 or h < 20:\n",
    "                continue\n",
    "            crop = img[y:y+h, x:x+w]\n",
    "            black_ratio = cv2.countNonZero(thresh[y:y+h, x:x+w]) / float(w*h)\n",
    "            if black_ratio < 0.6:\n",
    "                continue\n",
    "\n",
    "            inv = cv2.bitwise_not(crop)\n",
    "            try:\n",
    "                result = ocr.ocr(inv, cls=True)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if not result:\n",
    "                continue\n",
    "\n",
    "            # Safely extract all recognized text lines\n",
    "            lines = []\n",
    "            for block in result:\n",
    "                if not block:\n",
    "                    continue\n",
    "                for line in block:\n",
    "                    # line is typically [ [x1,y1], [text, conf], ... ]\n",
    "                    if len(line) >= 2 and isinstance(line[1], (list, tuple)):\n",
    "                        lines.append(line[1][0])\n",
    "            text = \" \".join(lines).strip()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # classify banner size\n",
    "            if h > 50:\n",
    "                super_headers.append(text)\n",
    "\n",
    "    # dedupe while preserving order\n",
    "    uniq_super = list(dict.fromkeys(super_headers))\n",
    "\n",
    "    return uniq_super\n",
    "\n",
    "def scan_folder_and_save_csv(root_folder : str, out_headers : str, check_folders : int = None):\n",
    "    all_super = []\n",
    "    \n",
    "    if check_folders is not None:\n",
    "        list_items = list(set(os.listdir(root_folder)) - set(['.DS_Store']))[0:check_folders]\n",
    "    else:\n",
    "        list_items = list(set(os.listdir(root_folder)) - set(['.DS_Store']))\n",
    "\n",
    "    for case in tqdm(sorted(list_items), desc=\"Folders\"):\n",
    "        case_path = os.path.join(root_folder, case)\n",
    "        if not os.path.isdir(case_path):\n",
    "            continue\n",
    "        # find the application PDF\n",
    "        excludes = {'acknowledgement', 'withdrawal', 'bvb', 'rdv', 'passport','summary'}\n",
    "        pdfs = [f for f in os.listdir(case_path) if (name := f.lower()).endswith(\".pdf\")\n",
    "                and \"application\" in name and not any(ex in name for ex in excludes)]\n",
    "        if not pdfs:\n",
    "            continue\n",
    "        pdf_path = os.path.join(case_path, pdfs[0])\n",
    "        sh = extract_banners_from_pdf(pdf_path)\n",
    "        all_super.extend(sh)\n",
    "    \n",
    "    # dedupe\n",
    "    uniq_super = sorted(set(all_super))\n",
    "    \n",
    "    # save\n",
    "    pd.Series(uniq_super, name=\"super_header\").to_csv(out_headers, index=False)\n",
    "    \n",
    "    print(f\"Saved {len(uniq_super)} super‐headers to {out_headers}\")\n",
    "    return uniq_super"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slugify(text: str) -> str:\n",
    "    t = unicodedata.normalize(\"NFKD\", text)\n",
    "    t = \"\".join(c for c in t if not unicodedata.combining(c))\n",
    "    t = t.lower()\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"_\", t).strip(\"_\")\n",
    "\n",
    "def initials(text: str) -> str:\n",
    "    return \"\".join(w[0] for w in text.split() if w).lower()\n",
    "\n",
    "def process_applications(root_folder: str, subheaders_list: list[str], check_folders : int = None):\n",
    "    sub_set = {s.lower() for s in subheaders_list}\n",
    "\n",
    "    rows = []\n",
    "    if check_folders is not None: \n",
    "        list_items = list(set(os.listdir(root_folder)) - set(['.DS_Store']))[0:check_folders]\n",
    "    else:\n",
    "        list_items = list(set(os.listdir(root_folder)) - set(['.DS_Store']))\n",
    "\n",
    "    for case in tqdm(sorted(list_items), desc=\"Cases\"):\n",
    "        case_path = os.path.join(root_folder, case)\n",
    "        if not os.path.isdir(case_path):\n",
    "            continue\n",
    "\n",
    "        # find application PDF\n",
    "        app_pdf = None\n",
    "        excludes = {'acknowledgement', 'withdrawal', 'bvb', 'passport', 'summary'}\n",
    "        for fn in os.listdir(case_path):\n",
    "            low = fn.lower()\n",
    "            if ( \"application\" in low and low.endswith(\".pdf\")  and not any(ex in low for ex in excludes)):\n",
    "                app_pdf = os.path.join(case_path, fn)\n",
    "                break\n",
    "        if not app_pdf:\n",
    "            continue\n",
    "\n",
    "        # collect all spans\n",
    "        doc = fitz.open(app_pdf)\n",
    "        spans = []\n",
    "        for page in doc:\n",
    "            for block in page.get_text(\"dict\")[\"blocks\"]:\n",
    "                if block[\"type\"] != 0:\n",
    "                    continue\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        font = span.get(\"font\", span.get(\"fontname\", \"\")).lower()\n",
    "                        spans.append({\n",
    "                            \"text\": span[\"text\"].strip(),\n",
    "                            \"size\": round(span[\"size\"], 1),\n",
    "                            \"flags\": span[\"flags\"],\n",
    "                            \"font\": font,\n",
    "                        })\n",
    "\n",
    "        # counters\n",
    "        ofm_count = 0\n",
    "        cv_count = 0\n",
    "        ehd_count = 0\n",
    "\n",
    "        # count occurrences of Employment history details anywhere\n",
    "        for s in spans:\n",
    "            if s[\"text\"].strip().lower() == \"employment history details\":\n",
    "                ehd_count += 1\n",
    "\n",
    "        current_sup = None\n",
    "        current_sub = None\n",
    "        rec = {\"sample_id\": case}\n",
    "\n",
    "        for i, s in enumerate(spans):\n",
    "            t = s[\"text\"]\n",
    "            if not t:\n",
    "                continue\n",
    "\n",
    "            bold = bool(s[\"flags\"] & 2) or (\"bold\" in s[\"font\"])\n",
    "            low = t.lower()\n",
    "\n",
    "            # super-header?  size=14.0 flags=16 bold font\n",
    "            if bold and s[\"size\"] == 14.0 and s[\"flags\"] == 16:\n",
    "                current_sup = t\n",
    "                current_sub = None\n",
    "                if low == \"other family members\":\n",
    "                    ofm_count += 1\n",
    "                if low == \"country visited\":\n",
    "                    cv_count += 1\n",
    "                continue\n",
    "\n",
    "            # sub-header?\n",
    "            if bold and low in sub_set:\n",
    "                current_sub = t\n",
    "                continue\n",
    "\n",
    "            # field/question?\n",
    "            is_field = (not bold) and (t.endswith(\"?\") or \":\" in t)\n",
    "            if is_field:\n",
    "                # skip if the question looks like page footer or layout artifact\n",
    "                if any(kw in t.lower() for kw in (\"generated\", \"reference number\", \"trn\")):\n",
    "                    continue\n",
    "                \n",
    "                # sanitize parentheses\n",
    "                key_txt = re.sub(r\"\\(.*?\\)\", \"X\", t)\n",
    "                base = key_txt.split(\":\", 1)[0].rstrip(\"?\").strip()\n",
    "                col_slug = slugify(base)\n",
    "                parts = [col_slug]\n",
    "                if current_sup:\n",
    "                    parts.append(initials(current_sup))\n",
    "                if current_sub:\n",
    "                    parts.append(initials(current_sub))\n",
    "                colname = \"_\".join(parts)\n",
    "\n",
    "                # find next bold span as valid answer\n",
    "                ans = \"\"\n",
    "                for nxt in spans[i + 1:]:\n",
    "                    nxt_bold = bool(nxt[\"flags\"] & 2) or (\"bold\" in nxt[\"font\"])\n",
    "                    val = nxt[\"text\"].strip()\n",
    "                    if nxt_bold and val and val.lower() not in (\"personal privacy\", \"official: sensitive\"):\n",
    "                        ans = val\n",
    "                        break\n",
    "                rec[colname] = ans\n",
    "                continue\n",
    "\n",
    "            # special: Value in Australian dollars\n",
    "            if \"value in australian dollars\" in low:\n",
    "                val = \"\"\n",
    "                for nxt in spans[i + 1:]:\n",
    "                    nxt_bold = bool(nxt[\"flags\"] & 2) or (\"bold\" in nxt[\"font\"])\n",
    "                    if nxt_bold and re.match(r\"^[\\d,\\.]+$\", nxt[\"text\"].replace(\" \", \"\")):\n",
    "                        val = nxt[\"text\"].strip()\n",
    "                        break\n",
    "                if val:\n",
    "                    rec[\"value_in_australian_dollars\"] = val\n",
    "                continue\n",
    "        \n",
    "        # add generation date from first page\n",
    "        generated_value = None\n",
    "        for span in spans[:100]:  # assuming ~100 spans on first page is enough\n",
    "            txt = span[\"text\"]\n",
    "            if txt.lower().startswith(\"generated:\"):\n",
    "                generated_value = txt.split(\":\", 1)[-1].strip()\n",
    "                break\n",
    "        rec[\"generated\"] = generated_value\n",
    "        \n",
    "        # store counts\n",
    "        rec[\"other_family_members_count\"] = ofm_count\n",
    "        rec[\"country_visited_count\"] = cv_count\n",
    "        rec[\"employment_history_details_count\"] = ehd_count\n",
    "\n",
    "        rows.append(rec)\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index(\"sample_id\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grant_list = pd.read_csv('header_Grant.csv')\n",
    "df_ref_list = pd.read_csv('header_Refusal.csv')\n",
    "\n",
    "headers_list = list(set(df_grant_list.super_header.tolist() + df_ref_list.super_header.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_case = 'Grant'\n",
    "#df_grant_list = pd.read_csv('header_Grant.csv')\n",
    "#list_headers_g = df_grant_list.super_header.tolist()\n",
    "##list_headers_g = scan_folder_and_save_csv('data_all/Info Students/'+type_case, 'header_'+type_case+'.csv', check_folders = 500)\n",
    "df_g = process_applications('data_all/Info Students/'+type_case,headers_list, check_folders = None)\n",
    "\n",
    "type_case = 'Refusal'\n",
    "#df_ref_list = pd.read_csv('header_Refusal.csv')\n",
    "#list_headers_r = df_ref_list.super_header.tolist()\n",
    "#list_headers_r = scan_folder_and_save_csv('data_all/Info Students/'+type_case, 'header_'+type_case+'.csv',check_folders = 300)\n",
    "df_r = process_applications('data_all/Info Students/'+type_case,headers_list, check_folders = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g['type_case'] = 'granted'\n",
    "df_r['type_case'] = 'refused'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = pd.concat([df_g,df_r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases.to_csv('data_onevisa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nans = pd.DataFrame(df_cases.isna().sum()).reset_index()\n",
    "df_nans.columns = ['cols_n','numbers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nans = df_nans.sort_values(by='numbers')\n",
    "df_nans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nans.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete_cols = df_nans[df_nans.numbers>=1600].cols_n.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df_cases.dropna(how='all')\n",
    "\n",
    "cleaned_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.type_case.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i in cleaned_df.columns.tolist() if 'e_ehd' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(cleaned_df.loc['Yaritza Solange Carvajal Perez'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_index = cleaned_df[(cleaned_df['other_family_members_count']==0) & (cleaned_df['country_visited_count']==0)  & (cleaned_df['employment_history_details_count']==0)][['type_case']].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_2 = cleaned_df.drop(index=remove_index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_2 = cleaned_df_2[cleaned_df_2.current_location_ac_cl.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBA???\n",
    "cleaned_df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nans = pd.DataFrame(cleaned_df_2.isna().sum()).reset_index()\n",
    "df_nans.columns = ['cols_n','numbers']\n",
    "df_nans = df_nans.sort_values(by='numbers')\n",
    "df_nans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_cols = df_nans[df_nans.numbers>=cleaned_df_2.shape[0]].cols_n.unique().tolist()\n",
    "cleaned_df_3 = cleaned_df_2.drop(columns=delete_cols+[''])\n",
    "cleaned_df_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nans = pd.DataFrame(cleaned_df_3.isna().sum()).reset_index()\n",
    "df_nans.columns = ['cols_n','numbers']\n",
    "df_nans = df_nans.sort_values(by='numbers')\n",
    "df_nans[df_nans.numbers>1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_3[cleaned_df_3.intelligence_agency_x_cd.isna()][['intelligence_agency_x_cd','type_case']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def print_exclusive_per_row_columns(df):\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    # Agrupa por prefijo sin sufijo (_cd, _od, _dvod, etc.)\n",
    "    for col in df.columns:\n",
    "        base = re.sub(r'_[a-z]{2,5}$', '', col)\n",
    "        groups[base].append(col)\n",
    "\n",
    "    for base, cols in groups.items():\n",
    "        if len(cols) < 2:\n",
    "            continue\n",
    "\n",
    "        sub_df = df[cols]\n",
    "        \n",
    "        # Para cada fila, contar cuántas columnas no son NaN\n",
    "        non_nan_counts_per_row = sub_df.notna().sum(axis=1)\n",
    "\n",
    "        # Si en todas las filas hay solo una columna con valor no NaN\n",
    "        if (non_nan_counts_per_row == 1).all():\n",
    "            print(f\"{base}: {cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_exclusive_per_row_columns(cleaned_df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_df_3[['authority_in_australia_cd', 'authority_in_australia_domt', 'authority_in_australia_doms', 'authority_in_australia_od', 'authority_in_australia_dvod']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def unify_exclusive_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    # Agrupar por base eliminando el sufijo (2–5 letras tras último guion bajo)\n",
    "    for col in df.columns:\n",
    "        base = re.sub(r'_[a-z]{2,5}$', '', col)\n",
    "        groups[base].append(col)\n",
    "\n",
    "    for base, cols in groups.items():\n",
    "        if len(cols) < 2:\n",
    "            continue\n",
    "\n",
    "        sub_df = df[cols]\n",
    "\n",
    "        # Check if only one non-NaN value per row\n",
    "        if (sub_df.notna().sum(axis=1) == 1).all():\n",
    "            # Create unified column with suffix _x\n",
    "            unified_col = base + \"_x\"\n",
    "            df[unified_col] = sub_df.bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "            # Drop original columns\n",
    "            df.drop(columns=cols, inplace=True)\n",
    "\n",
    "            print(f\"Unified: {cols} -> {unified_col}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "cleaned_df_4 = unify_exclusive_columns(cleaned_df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nans = pd.DataFrame(cleaned_df_4.isna().sum()).reset_index()\n",
    "df_nans.columns = ['cols_n','numbers']\n",
    "df_nans = df_nans.sort_values(by='numbers')\n",
    "df_nans[df_nans.numbers<=10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_4.to_csv('data_onevisa_postprocess.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_4.type_case.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame({\n",
    "        \"number_unique\": cleaned_df_4.nunique(),\n",
    "        \"number_nan\": cleaned_df_4.isna().sum(),\n",
    "        \"dtype\": cleaned_df_4.dtypes\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary[summary.number_unique>1400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.dtype.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.sort_values(by='number_unique').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary[(summary.number_unique<=200) & (summary.number_nan<=200)].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
